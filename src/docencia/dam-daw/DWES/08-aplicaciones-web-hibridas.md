---
title: UT08 Aplicaciones web h√≠bridas
icon: computer
---

# UT08 Aplicaciones web h√≠bridas: NUXT

> **En este tema trabajaremos los siguientes RAs:**
> RA9. Desarrolla aplicaciones Web h√≠bridas seleccionando y utilizando tecnolog√≠as, frameworks servidor y repositorios heterog√©neos de informaci√≥n.


## 1. Introducci√≥n: ¬øQu√© es una aplicaci√≥n web h√≠brida?

Una **aplicaci√≥n web h√≠brida** es aquella que combina caracter√≠sticas de las **aplicaciones web tradicionales** (que se ejecutan principalmente en el navegador y se desarrollan con tecnolog√≠as como HTML, CSS y JavaScript) con caracter√≠sticas de las **aplicaciones nativas** (que pueden instalarse y ejecutarse directamente en dispositivos m√≥viles o aprovechar funcionalidades del servidor).

En el contexto del **desarrollo web en entorno servidor**, una aplicaci√≥n h√≠brida:

* Se ejecuta parcialmente en el **servidor**, donde se procesan datos, se gestionan repositorios y se generan vistas din√°micas.
* Y parcialmente en el **cliente**, donde se aprovechan frameworks modernos (como **Nuxt 4**, **Next.js** o **SvelteKit**) que permiten ejecutar c√≥digo tanto en el servidor (SSR) como en el cliente (SPA).

Estas aplicaciones **recuperan y procesan informaci√≥n desde m√∫ltiples or√≠genes** (APIs REST, bases de datos, servicios externos, etc.) y utilizan **librer√≠as de terceros** para integrar funcionalidades espec√≠ficas (autenticaci√≥n, an√°lisis de datos, inteligencia de negocio, mapas, etc.).

En resumen, son h√≠bridas porque:

* Combinan **tecnolog√≠as de servidor y cliente**.
* Integran **fuentes de datos heterog√©neas**.
* Aprovechan **c√≥digo reutilizable** proveniente de librer√≠as y repositorios externos.
* Permiten **desplegar la misma base de c√≥digo** en diferentes entornos (web, m√≥vil, escritorio).


### 1.2. Ventajas de las aplicaciones h√≠bridas 

El desarrollo h√≠brido promueve la **reutilizaci√≥n del c√≥digo** y el **aprovechamiento de informaci√≥n ya existente**, lo que conlleva ventajas clave:

* **Eficiencia y rapidez en el desarrollo:** se pueden reutilizar m√≥dulos o componentes ya probados y mantenidos por la comunidad.
* **Compatibilidad multiplataforma:** el mismo desarrollo puede adaptarse a diferentes entornos sin duplicar esfuerzos.
* **Actualizaciones centralizadas:** al apoyarse en tecnolog√≠as web, las mejoras se aplican autom√°ticamente al servidor y al cliente.
* **Integraci√≥n de fuentes externas:** uso de APIs p√∫blicas (por ejemplo, OpenWeatherMap, Google Maps, o GitHub) para enriquecer la aplicaci√≥n.
* **Mejor mantenimiento y escalabilidad:** las librer√≠as y frameworks modernos facilitan la modularidad y la evoluci√≥n del proyecto.

### 1.3. Tecnolog√≠as y frameworks aplicables 

Para crear aplicaciones web h√≠bridas, se utilizan frameworks que soportan **Server-Side Rendering (SSR)** y **Static Site Generation (SSG)**, adem√°s de permitir ejecuci√≥n en el cliente (SPA):

* **Nuxt 4 (Vue 3):** framework h√≠brido para aplicaciones universales (server + client).
* **Next.js (React):** equivalente en el ecosistema React.
* **SvelteKit:** framework moderno orientado a rendimiento y SSR.
* **Angular Universal:** permite el renderizado en servidor con Angular.

Otros recursos clave:

* **APIs REST y GraphQL** para la comunicaci√≥n con fuentes externas.
* **Librer√≠as de terceros** como Axios, D3.js, Chart.js, Leaflet, TensorFlow.js, etc.
* **Bases de datos h√≠bridas** y servicios externos como Firebase, Supabase o MongoDB Atlas.


### **4. Creaci√≥n e integraci√≥n de repositorios de informaci√≥n (CE9_3 y CE9_4)**

Una aplicaci√≥n h√≠brida se caracteriza por **interactuar con repositorios de informaci√≥n existentes** y **crear sus propios repositorios derivados**.

Ejemplo:

* Recuperar datos de una **API p√∫blica** (por ejemplo, precios de criptomonedas o datos meteorol√≥gicos).
* Procesar esa informaci√≥n en el **servidor** (por ejemplo, filtrando o agregando datos).
* Almacenar los resultados en un **repositorio local o base de datos** (PostgreSQL, MongoDB‚Ä¶).
* Ofrecer una **API interna** para que otros servicios o usuarios consuman los nuevos datos generados.

---

### **5. Uso de librer√≠as de c√≥digo para funcionalidades espec√≠ficas (CE9_5)**

El uso de **librer√≠as** es esencial en el desarrollo h√≠brido. Estas librer√≠as ampl√≠an las capacidades de la aplicaci√≥n sin tener que desarrollar todo desde cero:

* **Autenticaci√≥n y seguridad:** JWT, Passport, Auth.js.
* **Visualizaci√≥n de datos:** Chart.js, ECharts, D3.js.
* **Mapas y geolocalizaci√≥n:** Leaflet, Mapbox.
* **Machine Learning e inteligencia de negocio:** TensorFlow.js, ML5.js.
* **Gesti√≥n de estado y composici√≥n:** Pinia, Vuex, Redux Toolkit.

---

### **6. Programaci√≥n basada en c√≥digo e informaci√≥n de terceros (CE9_6)**

Las aplicaciones h√≠bridas se apoyan en **ecosistemas abiertos de software**:

* Se integran **servicios web** (APIs de terceros).
* Se aprovecha **c√≥digo publicado** en plataformas como GitHub, npm o PyPI.
* Se adaptan **plantillas o componentes** que forman parte de frameworks de c√≥digo abierto.

Esto permite centrarse en el valor a√±adido del proyecto, reduciendo el tiempo de desarrollo y fomentando la interoperabilidad.

---

### **7. Inteligencia de datos y Big Data (CE9_7)**

En este √°mbito, las librer√≠as orientadas a la **inteligencia de negocios (BI)** y **Big Data** permiten que las aplicaciones h√≠bridas analicen datos de diferentes fuentes y generen conocimiento:

* **Librer√≠as de an√°lisis y visualizaci√≥n:** D3.js, Plotly, Chart.js.
* **Conectores de datos heterog√©neos:** APIs REST, CSV, JSON, bases SQL/NoSQL.
* **An√°lisis predictivo o dashboards:** integraci√≥n con servicios como Google Analytics, Power BI, o Apache Superset.

Estas herramientas ampl√≠an la funcionalidad de la aplicaci√≥n, transform√°ndola en una plataforma capaz de **procesar, analizar y representar datos en tiempo real**.

---

### **8. Prueba, depuraci√≥n y documentaci√≥n (CE9_8)**

El proceso final de desarrollo incluye:

* **Pruebas unitarias y de integraci√≥n** (con Jest, Vitest o Cypress).
* **Depuraci√≥n** mediante herramientas del navegador y consola del servidor.
* **Documentaci√≥n t√©cnica y de usuario**, que describa:

  * Arquitectura del sistema.
  * Dependencias y librer√≠as utilizadas.
  * APIs consultadas.
  * Estructura de carpetas y m√≥dulos.



```mermaid
flowchart TD

A["Cliente (Navegador) üåç"] -->|"Solicitud HTTP 1Ô∏è‚É£"| B["Servidor Nitro (Entorno servidor de Nuxt 4) üñ•Ô∏è"]

subgraph SERVIDOR["Entorno servidor - Nuxt 4 / Nitro üñ•Ô∏è"]
    B -->|"Ejecuta useFetch / useAsyncData"| C["API interna (server/api/...) üß©"]
    C -->|"Llama a API externa o BD 2Ô∏è‚É£"| D["API externa o Base de datos üåê üóÑÔ∏è"]
    D -->|"Devuelve datos JSON"| C
    C -->|"Datos procesados"| B
    B -->|"Renderizado SSR HTML + Datos 3Ô∏è‚É£"| E["P√°gina renderizada üìÑ"]
end

E -->|"Respuesta enviada al navegador 4Ô∏è‚É£"| A
A -->|"Vue 3 hidrata la p√°gina (interactividad SPA) 5Ô∏è‚É£"| F["Aplicaci√≥n h√≠brida activa ‚ö°"]
```


## Web Scrapping
::: note
Recuperado de [Manu Perez Alonso](https://manu-perez-alfonso.github.io/modulos/01-Servidor/ud11/#evaluacion)
:::

Internet representa una vasta y valiosa fuente de datos en numerosas √°reas de inter√©s. Existen diferentes posibilidades a trav√©s de las cuales recolectar datos a trav√©s de Internet:

* Juegos de datos ya existentes:

  * P√∫blicos: existen numerosos conjuntos de datos ya preparados para entrenar diferentes algoritmos de Inteligencia Artificial. Puedes encontrar algunos de estos ejemplos en este enlace.
  * Compra de juegos de datos: en numerosas plataformas es posible comprar juegos de datos sobre diversas tem√°ticas: consumo, medioambiente, pol√≠tica... Un ejemplo lo puedes encontrar en este enlace.
  * Datos corporativos: son los datos transaccionales o agregados, generados por la propia actividad privada de una empresa u organizaci√≥n.

* Creaci√≥n de juegos de datos:
  * Generaci√≥n de datos: mediante la creaci√≥n de encuestas, o la utilizaci√≥n de servicios como AmazonTurk, que permiten contratar personal para tareas como etiquetado de datos y clasificaci√≥n.
  * Recolecci√≥n de datos existentes: mediante servicios API REST.
  
En la unidad anterior vimos c√≥mo recuperar datos de diversas API REST, dise√±adas para tal efecto, pero, ¬øqu√© ocurre si existen sitios web que no proporcionen un servicio web similar?

Para estos casos se puede utilizar Web Scraping, que se trata de una t√©cnica consistente en extraer datos del c√≥digo HTML de los sitios web. Antes de aplicar esta t√©cnica a un sitio web, es necesario tener en consideraci√≥n determinados factores:

Legales

¬øSe incumple alg√∫n reglamento nacional/regional?
¬øSe incumplen los "T√©rminos y condiciones" del sitio web?
¬øSe est√° accediendo a lugares no autorizados?
¬øEs legal el uso que se le dar√° a los datos?
√âticos

Robots.txt: es un fichero con informaci√≥n para que los motores de b√∫squeda no indexen determinadas p√°ginas de un sitio web. Para acceder a este fichero, se a√±ade "/robots.txt" al final de un determinado dominio, de la forma "dominio.com/robots.txt". Aqu√≠ se detallan aspectos como si se permite/deniega acceso total o parcial, frecuencia de consultas (crawl-delay), el sitemap (para facilitar la navegaci√≥n por el sitio), etc. Para m√°s informaci√≥n, consulta este enlace. No respetar las normas establecidas en este fichero puede acarrear consecuencias legales.
En caso de dudas sobre si se puede aplicar esta t√©cnica a un determinado sitio web, el mejor consejo es contactar con la empresa/organizaci√≥n y preguntar.

Tipos de sitios web¬∂
Existen diferentes casu√≠sticas que nos podemos encontrar cuando tratamos de aplicar Web Scraping en un sitio web, dependiendo del paradigma en que est√© basado el sitio web en cuesti√≥n:

HTML pre-renderizado o sitios web est√°ticos: se trata de sitios web cuyo c√≥digo HTML se env√≠a desde el servidor (backend), ya sea porque se trata de un sitio web est√°tico, porque se utilizan frameworks con sistemas de plantillas (Laravel, CodeIgniter, Django...) que embeben c√≥digo de servidor en HTML, o tambi√©n porque se utiliza la t√©cnica de SSR en aplicaciones web reactivas (Vue, React, Angular...). En este caso el HTML se obtiene al hacer una petici√≥n HTTP, y se pueden utilizar librer√≠as como BeautifulSoup4 o Scrapy.

Single Page Application (SPA): consiste en un fichero HTML simple con c√≥digo JavaScript asociado. Durante la navegaci√≥n, el navegador web ejecuta el c√≥digo JavaScript y modifica din√°micamente el c√≥digo HTML para liberar al servidor de esta tarea. Los datos se descargan mediante peticiones HTTP a servicios REST que residen en el servidor, actualiz√°ndose solo la parte del HTML que se necesita, sin originar un refresco de toda la p√°gina web. Al hacer una petici√≥n HTTP, lo que se obtiene es el HTML simple y no los datos que se visualizan en pantalla (ya que es el c√≥digo JavaScript, en el lado cliente, quien modifica el HTML, tras hacerse la petici√≥n HTTP). En este caso existen dos aproximaciones para Web Scraping:

Utilizar una herramienta como Selenium, para simular un navegador web que accede al sitio web, y as√≠ poder ejecutar el c√≥digo JavaScript que genere el c√≥digo HTML. Selenium se trata realmente de un entorno de pruebas para aplicaciones web, aunque su uso ha derivado tambi√©n hacia el Web Scraping. Adem√°s de para SPAs, Selenium tambi√©n se puede utilizar para los sitios web de la tipolog√≠a anterior.
Inspeccionar las peticiones HTTP que se realizan al backend para descubrir los endpoints, y as√≠ poder realizar peticiones HTTP directamente a esos endpoints y recuperar los datos en formato JSON (generalmente).
Adem√°s de estas consideraciones, ser√° necesario establecer si se requiere alg√∫n tipo de autenticaci√≥n al realizar la petici√≥n HTTP.

Ejemplos de Web Scraping¬∂
A continuaci√≥n vamos a desarrollar dos ejemplos de Web Scraping, uno con BeautifulSoup4, y otro con Selenium. Para ello, previamente deberemos instalar en nuestro entorno virtual los correspondientes paquetes:


pip install beautifulsoup4
pip install selenium
Fake jobs - BeautifulSoup4¬∂
En este ejemplo vamos a extraer determinados datos de una web con ofertas de trabajo falsas. La URL de la cual vamos a extraer estos datos es:

https://realpython.github.io/fake-jobs/

Y los datos de cada oferta son:

T√≠tulo
Compa√±√≠a
Ubicaci√≥n
Una vez tenemos claro los datos que necesitamos extraer, as√≠ como la URL donde encontrar dichos datos, deber√≠amos plantearnos si se incumple alg√∫n tipo de norma tanto legal como √©tica (seg√∫n las consideraciones mencionadas en el apartado introductorio). Al tratarse de un sitio de pruebas espec√≠ficamente dise√±ado para practicar Web Scraping, no tenemos ning√∫n impedimento para continuar con las pruebas.

Como segundo paso a realizar (previo a la programaci√≥n del script o bot), es esencial analizar la estructura del sitio web del que queremos descargar los datos. Para ello, debemos inspeccionar el c√≥digo HTML con el navegador que estemos utilizando. En el caso de Google Chrome, pulsamos bot√≥n derecho del rat√≥n sobre el elemento a inspeccionar, y pulsamos sobre "Inspeccionar", tras lo cual nos aparecer√°n las herramientas de desarrollador, sobre la pesta√±a "Elementos", y apuntando directamente al elemento sobre el que hab√≠amos pulsado el bot√≥n derecho del rat√≥n:



Volveremos a esta estructura m√°s adelante, pero ahora vamos a introducir las primeras instrucciones de c√≥digo:


import requests
from bs4 import BeautifulSoup

URL = "https://realpython.github.io/fake-jobs/"
page = requests.get(URL)
soup = BeautifulSoup(page.content, "html.parser")
La primera diferencia con respecto a lo que hemos hecho hasta ahora es la importaci√≥n de la clase BeautifulSoup de la librer√≠a correspondiente. Adem√°s, hemos creado un objeto BeautifulSoup a partir del resultado devuelto por la petici√≥n HTTP a la URL donde se encuentran los datos.
El segundo par√°metro "html.parser" indica el tipo de parser que se va a utilizar. Un parser servir√° para poder distinguir entre los distintos elementos de un documento basado en lenguaje de marcas (HTML en este caso) para poder as√≠ navegar por ellos posteriormente. En el caso de BeautifulSoup, podemos encontrar tres tipos:

html.parser: se trata del parser que viene por defecto instalado en la librer√≠a est√°ndar de Python.
lxml: combina caracter√≠sticas de XML y se caracteriza por su rapidez.
html5lib: se caracteriza por interpretar el HTML del mismo modo que un navegador web.
En general, utilizar√≠amos lxml cuando necesit√°semos rapidez. Adem√°s, para versiones de Python igual o anteriores a la 3.2.2, se recomienda usar lxml o html5lib. Para saber m√°s sobre las diferencias de estos parsers, se puede consultar este enlace.

Encontrar elementos por ID¬∂
Tras inspeccionar el c√≥digo HTML seg√∫n se ha descrito anteriormente, vemos que las ofertas de trabajo est√°n contenidas en elementos div con clase \"card\", a su vez contenido en un div con clases \"column is-half\". Todos estos elementos div est√°n a su vez contenidos en un elemento div con el atributo id con valor \"ResultsContainer\". Por tanto, √©ste es el primer elemento a partir del cual empezar la b√∫squeda:



La instrucci√≥n para realizar esto ser√°:


results = soup.find(id="ResultsContainer")
Ahora el objeto results va a contener el elemento con id "ResultsContainer", y todos los contenidos dentro de √©l. El m√©todo find recuperar√° un solo elemento.
Si quisi√©semos ver de forma "amigable" el HTML obtenido, podemos utilizar la siguiente instrucci√≥n:


print(results.prettify())
Encontrar elementos por etiqueta y clase¬∂
Una vez hemos acotado la parte del documento que contiene los datos que nos interesan, vamos a localizar exactamente d√≥nde se encuentran dichos datos:



Por tanto, se puede apreciar que los datos que necesitamos se encuentran dentro de un elemento div con clase "card-content". Para poder recuperar todos los elementos con esta clase, utilizamos la siguiente instrucci√≥n:


job_elements = results.find_all("div", class_="card-content")
De esta forma obtenemos otro objeto BeautifulSoup llamado "job_elements" a partir de "results". Si no hubi√©semos especificado el atributo class_, habr√≠amos recuperado todos los elementos div del documento.
Con el m√©todo find_all vamos a obtener un iterable que podremos recorrer con un bucle:


for job_element in job_elements:
    title_element = job_element.find("h2", class_="title")
    company_element = job_element.find("h3", class_="company")
    location_element = job_element.find("p", class_="location")
    print(title_element)
    print(company_element)
    print(location_element)
    print()
NOTA: Si hubi√©semos utilizado el m√©todo find en lugar de find_all, solo habr√≠amos recuperado el primero de los elementos del √°rbol con las caracter√≠sticas especificadas.

Por cada job_element (que tambi√©n es un objeto de BeautifulSoup) dentro de job_elements, buscaremos diferentes elementos (h2, h3 y p) con sus correspondientes clases (title, company y location). A continuaci√≥n imprimimos sus valores, y una l√≠nea en blanco al final de cada bloque.

La salida del bucle tiene la siguiente apariencia:


<h2 class="title is-5">Senior Python Developer</h2>
<h3 class="subtitle is-6 company">Payne, Roberts and Davis</h3>
<p class="location">Stewartbury, AA</p>
Para redondear esta primera prueba, vamos a extraer el texto de los elementos anteriores mediante la funci√≥n get_text(), y el resultado lo concatenaremos con la funci√≥n strip() para extraer los posibles espacios en blanco. Con lo que el script quedar√≠a del siguiente modo:


import requests
from bs4 import BeautifulSoup
URL = "https://realpython.github.io/fake-jobs/"
page = requests.get(URL)
soup = BeautifulSoup(page.content, "html.parser")
results = soup.find(id="ResultsContainer")
job_elements = results.find_all("div", class_="card-content")
for job_element in job_elements:
    title_element = job_element.find("h2", class_="title")
    company_element = job_element.find("h3", class_="company")
    location_element = job_element.find("p", class_="location")
    print(title_element.get_text().strip())
    print(company_element.get_text().strip())
    print(location_element.get_text().strip())
    print()
Para lanzar este script, lo guardamos primero en un fichero con extensi√≥n .py (por ejemplo fake_jobs_scraping.py), seguidamente activamos el entorno virtual desde el terminal, y lo ejecutamos de la forma:


(venv) usuario: python /"ruta hasta el fichero"/fake_jobs_scraping.py
Si accedemos al directorio donde se encuentra el fichero (mediante el comando "cd"), simplemente lo podemos ejecutar sin especificar la ruta:

(venv) usuario: python fake_jobs_scraping.py